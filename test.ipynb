{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/k64t/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/k64t/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "import os.path as osp\n",
    "import logging\n",
    "import torch\n",
    "from utils import read_image\n",
    "from utils.simple_tokenizer import SimpleTokenizer\n",
    "from prettytable import PrettyTable\n",
    "import random\n",
    "import regex as re\n",
    "import copy\n",
    "import torchvision.transforms as T\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "def tokenize(caption: str, tokenizer, text_length=77, truncate=True) -> torch.LongTensor:\n",
    "    sot_token = tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    tokens = [sot_token] + tokenizer.encode(caption) + [eot_token]\n",
    "\n",
    "    result = torch.zeros(text_length, dtype=torch.long)\n",
    "    if len(tokens) > text_length:\n",
    "        if truncate:\n",
    "            tokens = tokens[:text_length]\n",
    "            tokens[-1] = eot_token\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"Input {caption} is too long for context length {text_length}\"\n",
    "            )\n",
    "    result[:len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "def build_random_masked_tokens_and_labels(tokens):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of int, tokenized sentence.\n",
    "    :return: (list of int, list of int), masked tokens and related labels for MLM prediction\n",
    "    \"\"\"\n",
    "    mask = tokenizer.encoder[\"<|mask|>\"]\n",
    "    token_range = list(range(1, len(tokenizer.encoder)-3)) # 1 ~ 49405\n",
    "    \n",
    "    labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if 0 < token < 49405:\n",
    "            prob = random.random()\n",
    "            # mask token with 15% probability\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = mask\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.choice(token_range)\n",
    "\n",
    "                # -> rest 10% randomly keep current token\n",
    "\n",
    "                # append current token to output (we will predict these later)\n",
    "                labels.append(token)\n",
    "            else:\n",
    "                # no masking token (will be ignored by loss function later)\n",
    "                labels.append(0)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    \n",
    "    if all(l == 0 for l in labels):\n",
    "        # at least mask 1\n",
    "        labels[1] = tokens[1]\n",
    "        tokens[1] = mask\n",
    "\n",
    "    return torch.tensor(tokens), torch.tensor(labels)\n",
    "\n",
    "def build_random_masked_tokens_and_labels_2(tokens):\n",
    "    \"\"\"\n",
    "    Masking some (nouns, adjective, verb) tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of int, tokenized sentence.\n",
    "    :return: (list of int, list of int), masked tokens and related labels for MLM prediction\n",
    "    \"\"\"\n",
    "    mask = tokenizer.encoder[\"<|mask|>\"]\n",
    "    token_range = list(range(1, len(tokenizer.encoder)-3)) # 1 ~ 49405\n",
    "    selected_categories= ['DT', \"NN\", \"NNS\", \"JJ\", \"CD\", \"PRP\"]\n",
    "\n",
    "    def __is_core_words_(token):\n",
    "        try:\n",
    "            if not token in token_range: return False\n",
    "            word = tokenizer.decode([int(token)])\n",
    "            tokens = nltk.word_tokenize(word)\n",
    "            post_tag = nltk.pos_tag(tokens)[0][1]\n",
    "            return post_tag in selected_categories\n",
    "        except:\n",
    "            print(\"mlm2 got error in nltk --> \",word, \" \\t pos_tag = \", nltk.pos_tag([word]))\n",
    "            return True     \n",
    "\n",
    "\n",
    "    labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        label_token = int(token)\n",
    "        if 0 < token < 49405 and __is_core_words_(token):\n",
    "            prob = random.random()\n",
    "            if prob < 0.15:\n",
    "                prob /= 0.15\n",
    "                \n",
    "\n",
    "                # 80% randomly change token to mask token\n",
    "                if prob < 0.8:\n",
    "                    tokens[i] = mask\n",
    "\n",
    "                # 10% randomly change token to random token\n",
    "                elif prob < 0.9:\n",
    "                    tokens[i] = random.choice(token_range)\n",
    "\n",
    "                # -> rest 10% randomly keep current token\n",
    "\n",
    "                # append current token to output (we will predict these later)\n",
    "                labels.append(label_token)\n",
    "            else:\n",
    "                # no masking token (will be ignored by loss function later)\n",
    "                labels.append(0)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    \n",
    "    if all(l == 0 for l in labels):\n",
    "        # at least mask 1\n",
    "        labels[1] = tokens[1]\n",
    "        tokens[1] = mask\n",
    "\n",
    "    return torch.tensor(tokens), torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([49406,   518,   786,   533,  3309,  2866, 13135,  6116,   269,   797,\n",
       "          791,  3005,  3144,  2225,   269,   797,   533,  3309,   320,  7048,\n",
       "          339,  2523,   593,   320,   736,  8504,   269,   797,   791,   320,\n",
       "        14894,   593,   637,  4481, 13946,   537,   637,  1746, 13946,   269,\n",
       "          797,   791,   525, 41646,  5003,   537,  1449,  4079,   593,  4481,\n",
       "        14182,   269,   320,  2533,  3309,  2866,  5034,  6116,  7286,   320,\n",
       "         3467,   268,   537,   268,  1449, 14115,   530,   518,  1823,  2463,\n",
       "         1519,  3309,   320,  7048,   339,   268, 49407])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption = \"The man is wearing brown framed glasses. He has short dark hair. He is wearing a gray t shirt with a red pocket. He has a backpack with one yellow strap and one blue strap. He has on khaki pants and black shoes with yellow trim. A person wearing brown eyeglasses holds a silver-and-black object in the left hand while wearing a gray t-shirt with a yellow backpack strap over one shoulder and a blue strap over the other shoulder. The person is wearing brown pants ending above the ankle with black and lime-green running shoes.\"\n",
    "tokens_1 = tokenize(caption, tokenizer)\n",
    "tokens_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207183/1370727398.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(tokens), torch.tensor(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([49406,   518,   786,   533,  3309,  2866, 49405, 49405,   269,   797,\n",
       "           791,  3005,  3144,  2225,   269,   797,   533,  3309,   320,  7048,\n",
       "         49405, 49405,   593,   320,   736,  8504,   269,   797, 49405,   320,\n",
       "         14894,   593,   637,  4481, 13946,   537,   637,  1746, 13946,   269,\n",
       "           797,   791,   525, 41646,  5003,   537, 24749,  4079,   593,  4481,\n",
       "         14182,   269,   320,  2533,  3309,  2866, 49405,  6116,  7286,   320,\n",
       "         21162,   268,   537,   268,  1449, 14115,   530,   518,  1823,  2463,\n",
       "          1519,  3309,   320,  7048,   339,   268, 49407]),\n",
       " tensor([    0,     0,     0,     0,     0,     0, 49405, 49405,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         49405, 49405,     0,     0,     0,     0,     0,     0, 49405,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 24749,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 49405,     0,     0,     0,\n",
       "         21162,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,  7048,     0,     0,     0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_random_masked_tokens_and_labels(tokens_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207183/3374751453.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(tokens), torch.tensor(labels)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([49406,   518, 49405,   533,  3309,  2866, 49405, 49405,   269,   797,\n",
       "           791,  3005,  3144,  2225,   269,   797,   533,  3309,   320, 12867,\n",
       "         49405, 49405,   593,   320, 49405,  8504,   269,   797, 49405,   320,\n",
       "         45485,   593,   637,  4481, 13946,   537,   637,  1746, 13946,   269,\n",
       "           797,   791,   525, 41646,  5003,   537, 24749,  4079,   593, 49405,\n",
       "         14182,   269,   320,  2533,  3309,  2866, 49405,  6116,  7286,   320,\n",
       "         21162,   268,   537,   268,  1449, 14115,   530,   518,  1823,  2463,\n",
       "          1519,  3309,   320,  7048,   339,   268, 49407]),\n",
       " tensor([    0,     0,   786,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,  7048,\n",
       "             0,     0,     0,     0,   736,     0,     0,     0,     0,     0,\n",
       "         14894,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,  4481,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,  1449,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_ids, mlm_label = build_random_masked_tokens_and_labels_2(tokens_1.clone().detach())\n",
    "mlm_ids, mlm_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked indices:\n",
      "tensor([False, False, False,  True, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True, False, False,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False,  True, False, False, False, False, False, False, False,  True,\n",
      "         True, False, False, False, False, False, False,  True, False, False,\n",
      "         True, False, False, False, False,  True, False, False,  True, False,\n",
      "        False, False, False, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False,  True, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([49406, 32678,   786,   533,    -1,    -1, 13135,    -1,   269,    -1,\n",
       "            -1,    -1,    -1,  2225, 49405, 49405,    -1,    -1,    -1,    -1,\n",
       "           339,  2523,    -1, 24598,    -1,    -1,    -1, 20968,   791,   320,\n",
       "         49405,    -1,    -1,    -1, 13946, 46302,    -1,  1746,    -1,    -1,\n",
       "            -1,    -1, 22716,    -1,  5003, 49405, 49405,    -1,   593, 35150,\n",
       "            -1,   269,    -1,    -1,  3309, 26858,  5034,  6116,    -1,    -1,\n",
       "          3467,   268,    -1, 49405,    -1, 14115,   530,    -1,    -1,  2463,\n",
       "            -1,  3309,    -1,  7048,   339,   268, 49407]),\n",
       " tensor([ -100,  -100,  -100,   533,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,   533,  -100,  -100,    -1,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,   593,  -100,  -100,  -100,  -100,  -100,  -100,  -100,    -1,\n",
       "         49405,  -100,  -100,  -100,  -100,  -100,  -100, 49405,  -100,  -100,\n",
       "            -1,  -100,  -100,  -100,  -100,  2866,  -100,  -100,    -1,  -100,\n",
       "          -100,  -100,  -100,  -100,    -1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,   268,  -100]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer()\n",
    "vocal_size = 49408\n",
    "labels = tokens_1.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.25)\n",
    "def mask(input_ids, vocab_size, targets=None, masked_indices=None, probability_matrix=None):\n",
    "    if masked_indices is None:\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    masked_indices[input_ids > 49405] = False\n",
    "    print(\"masked indices:\")\n",
    "    print(masked_indices)\n",
    "    if targets is not None:\n",
    "        targets[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids[indices_replaced] = -1 #tokenizer.encoder[\"<|mask|>\"]\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(input_ids.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(vocab_size, input_ids.shape, dtype=torch.long).to(input_ids.device)\n",
    "    input_ids[indices_random] = random_words[indices_random]\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    if targets is not None:\n",
    "        return input_ids, targets\n",
    "    else:\n",
    "        return input_ids\n",
    "mask(tokens_1, vocab_size=vocal_size, targets=labels, probability_matrix=probability_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "\tm, n = x.size(0), y.size(0)\n",
    "\txx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "\tyy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "\tdist = xx + yy\n",
    "\tdist.addmm_(1, -2, x, y.t())\n",
    "\tdist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
    "\treturn dist\n",
    "\n",
    "def cosine_dist(x, y):\n",
    "\tbs1, bs2 = x.size(0), y.size(0)\n",
    "\tfrac_up = torch.matmul(x, y.transpose(0, 1))\n",
    "\tfrac_down = (torch.sqrt(torch.sum(torch.pow(x, 2), 1))).view(bs1, 1).repeat(1, bs2) * \\\n",
    "\t            (torch.sqrt(torch.sum(torch.pow(y, 2), 1))).view(1, bs2).repeat(bs1, 1)\n",
    "\tcosine = frac_up / frac_down\n",
    "\treturn 1-cosine\n",
    "\n",
    "def _batch_hard(mat_distance, mat_similarity, indice=False, topK=0):\n",
    "\t#topK should be < than  the number instances /id in a batch\n",
    "\tsorted_mat_distance, positive_indices = torch.sort(mat_distance + (-9999999.) * (1 - mat_similarity), dim=1, descending=True)\n",
    "\thard_p = sorted_mat_distance[:, :topK]\n",
    "\thard_p_indice = positive_indices[:, :topK]\n",
    "\tsorted_mat_distance, negative_indices = torch.sort(mat_distance + (9999999.) * (mat_similarity), dim=1, descending=False)\n",
    "\thard_n = sorted_mat_distance[:, :topK]\n",
    "\thard_n_indice = negative_indices[:, :topK]\n",
    "\tif(indice):\n",
    "\t\treturn hard_p, hard_n, hard_p_indice, hard_n_indice\n",
    "\treturn hard_p, hard_n\n",
    "\n",
    "\n",
    "\n",
    "class TopKTripletLoss(nn.Module):\n",
    "\n",
    "\tdef __init__(self, margin=0, normalize_feature=False, skip_mean=False, topK=1):\n",
    "\t\tsuper(TopKTripletLoss, self).__init__()\n",
    "\t\tself.margin = margin\n",
    "\t\tself.normalize_feature = normalize_feature\n",
    "\t\tself.skip_mean = skip_mean\n",
    "\t\tself.topk = topK\n",
    "\tdef forward(self, emb1, emb2, label):\n",
    "\t\tif self.normalize_feature:\n",
    "\t\t\t# equal to cosine similarity\n",
    "\t\t\temb1 = F.normalize(emb1)\n",
    "\t\t\temb2 = F.normalize(emb2)\n",
    "\n",
    "\t\tmat_dist = euclidean_dist(emb1, emb2)\n",
    "\t\tassert mat_dist.size(0) == mat_dist.size(1)\n",
    "\t\tN = mat_dist.size(0)\n",
    "\t\tmat_sim = label.expand(N, N).eq(label.expand(N, N).t()).float()\n",
    "\n",
    "\t\tdist_ap, dist_an, ap_idx, an_idx = _batch_hard(mat_dist, mat_sim, indice=True, topK=self.topk)\n",
    "\n",
    "\t\tassert an_idx.size(0)==ap_idx.size(0)\n",
    "\t\tdist_group_ap = torch.sum((emb1 - torch.mean(emb2[ap_idx], dim=1)) ** 2, dim=1).sqrt()\n",
    "\t\tdist_group_an = torch.sum((emb1 - torch.mean(emb2[an_idx], dim=1)) ** 2, dim=1).sqrt()\n",
    "\n",
    "\t\t# triple_dist = torch.stack((dist_ap, dist_an), dim=1)\n",
    "\t\ttriple_dist = torch.stack((dist_group_ap, dist_group_an), dim=1)\n",
    "\t\tprint(triple_dist.shape)\n",
    "\t\ttriple_dist = F.log_softmax(triple_dist, dim=1)\n",
    "\n",
    "\t\tloss = (- self.margin * triple_dist[:,0] - (1 - self.margin) * triple_dist[:,1])\n",
    "\t\tif self.skip_mean:\n",
    "\t\t\treturn loss\n",
    "\t\telse:\n",
    "\t\t\treturn loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 2, 0, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0,\n",
      "        2, 1, 2, 0, 2, 2, 0, 0])\n",
      "torch.Size([32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9422)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = torch.rand(32, 2)\n",
    "label = torch.randint(low=0, high=3, size=(32,))\n",
    "print(label)\n",
    "loss = TopKTripletLoss(margin=0, topK=4)\n",
    "loss(em, em, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/TinyZeaMays/CircleLoss/blob/master/circle_loss.py \n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "\n",
    "def convert_label_to_similarity(normed_feature: Tensor, label: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    similarity_matrix = normed_feature @ normed_feature.transpose(1, 0)\n",
    "    label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
    "\n",
    "    positive_matrix = label_matrix.triu(diagonal=1)  #set diagnoal from 1 to 0, exclude itself\n",
    "    negative_matrix = label_matrix.logical_not().triu(diagonal=1) #set diagnoal from 0 to 1, exclude itself\n",
    "\n",
    "    similarity_matrix = similarity_matrix.view(-1)\n",
    "    positive_matrix = positive_matrix.view(-1)\n",
    "    negative_matrix = negative_matrix.view(-1)\n",
    "    return similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]\n",
    "\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, m: float, gamma: float) -> None:\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.m = m\n",
    "        self.gamma = gamma\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "    def forward(self, sp: Tensor, sn: Tensor) -> Tensor:\n",
    "        ap = torch.clamp_min(- sp.detach() + 1 + self.m, min=0.)\n",
    "        an = torch.clamp_min(sn.detach() + self.m, min=0.)\n",
    "\n",
    "        delta_p = 1 - self.m\n",
    "        delta_n = self.m\n",
    "\n",
    "        logit_p = - ap * (sp - delta_p) * self.gamma\n",
    "        logit_n = an * (sn - delta_n) * self.gamma\n",
    "\n",
    "        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(423.2520, grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "feat = nn.functional.normalize(torch.rand(16, 2, requires_grad=True))\n",
    "label = torch.randint(high=3, size=(16,))\n",
    "\n",
    "inp_sp, inp_sn = convert_label_to_similarity(feat, label)\n",
    "\n",
    "criterion = CircleLoss(m=0.25, gamma=256)\n",
    "circle_loss = criterion(inp_sp, inp_sn)\n",
    "\n",
    "print(circle_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9697, 0.6768, 0.9319, 0.9713, 0.7001, 0.8127, 0.9014, 0.8106,\n",
      "         0.9989, 0.9152, 0.9129, 0.9979, 0.9953, 0.9996, 0.8397],\n",
      "        [0.9697, 1.0000, 0.8361, 0.8151, 0.8838, 0.8533, 0.6457, 0.9799, 0.6430,\n",
      "         0.9800, 0.7891, 0.9850, 0.9518, 0.9415, 0.9621, 0.9469],\n",
      "        [0.6768, 0.8361, 1.0000, 0.3638, 0.4824, 0.9995, 0.1210, 0.9288, 0.1176,\n",
      "         0.7104, 0.3228, 0.9184, 0.6275, 0.6024, 0.6550, 0.9681],\n",
      "        [0.9319, 0.8151, 0.3638, 1.0000, 0.9914, 0.3935, 0.9687, 0.6830, 0.9678,\n",
      "         0.9140, 0.9990, 0.7027, 0.9535, 0.9627, 0.9422, 0.5856],\n",
      "        [0.9713, 0.8838, 0.4824, 0.9914, 1.0000, 0.5103, 0.9279, 0.7726, 0.9266,\n",
      "         0.9592, 0.9848, 0.7897, 0.9847, 0.9898, 0.9779, 0.6865],\n",
      "        [0.7001, 0.8533, 0.9995, 0.3935, 0.5103, 1.0000, 0.1528, 0.9402, 0.1494,\n",
      "         0.7326, 0.3530, 0.9306, 0.6522, 0.6277, 0.6789, 0.9756],\n",
      "        [0.8127, 0.6457, 0.1210, 0.9687, 0.9279, 0.1528, 1.0000, 0.4802, 1.0000,\n",
      "         0.7846, 0.9786, 0.5040, 0.8488, 0.8652, 0.8294, 0.3658],\n",
      "        [0.9014, 0.9799, 0.9288, 0.6830, 0.7726, 0.9402, 0.4802, 1.0000, 0.4772,\n",
      "         0.9206, 0.6505, 0.9996, 0.8713, 0.8552, 0.8883, 0.9920],\n",
      "        [0.8106, 0.6430, 0.1176, 0.9678, 0.9266, 0.1494, 1.0000, 0.4772, 1.0000,\n",
      "         0.7825, 0.9779, 0.5010, 0.8470, 0.8635, 0.8274, 0.3626],\n",
      "        [0.9989, 0.9800, 0.7104, 0.9140, 0.9592, 0.7326, 0.7846, 0.9206, 0.7825,\n",
      "         1.0000, 0.8955, 0.9309, 0.9938, 0.9897, 0.9971, 0.8640],\n",
      "        [0.9152, 0.7891, 0.3228, 0.9990, 0.9848, 0.3530, 0.9786, 0.6505, 0.9779,\n",
      "         0.8955, 1.0000, 0.6710, 0.9395, 0.9499, 0.9266, 0.5497],\n",
      "        [0.9129, 0.9850, 0.9184, 0.7027, 0.7897, 0.9306, 0.5040, 0.9996, 0.5010,\n",
      "         0.9309, 0.6710, 1.0000, 0.8844, 0.8691, 0.9005, 0.9882],\n",
      "        [0.9979, 0.9518, 0.6275, 0.9535, 0.9847, 0.6522, 0.8488, 0.8713, 0.8470,\n",
      "         0.9938, 0.9395, 0.8844, 1.0000, 0.9995, 0.9994, 0.8026],\n",
      "        [0.9953, 0.9415, 0.6024, 0.9627, 0.9898, 0.6277, 0.8652, 0.8552, 0.8635,\n",
      "         0.9897, 0.9499, 0.8691, 0.9995, 1.0000, 0.9977, 0.7831],\n",
      "        [0.9996, 0.9621, 0.6550, 0.9422, 0.9779, 0.6789, 0.8294, 0.8883, 0.8274,\n",
      "         0.9971, 0.9266, 0.9005, 0.9994, 0.9977, 1.0000, 0.8234],\n",
      "        [0.8397, 0.9469, 0.9681, 0.5856, 0.6865, 0.9756, 0.3658, 0.9920, 0.3626,\n",
      "         0.8640, 0.5497, 0.9882, 0.8026, 0.7831, 0.8234, 1.0000]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [False,  True, False, False,  True, False, False, False,  True,  True,\n",
      "         False, False, False, False,  True, False],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False,  True],\n",
      "        [ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [False,  True, False, False,  True, False, False, False,  True,  True,\n",
      "         False, False, False, False,  True, False],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False,  True],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False,  True],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False,  True],\n",
      "        [False,  True, False, False,  True, False, False, False,  True,  True,\n",
      "         False, False, False, False,  True, False],\n",
      "        [False,  True, False, False,  True, False, False, False,  True,  True,\n",
      "         False, False, False, False,  True, False],\n",
      "        [ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [ True, False, False,  True, False, False, False, False, False, False,\n",
      "          True,  True,  True,  True, False, False],\n",
      "        [False,  True, False, False,  True, False, False, False,  True,  True,\n",
      "         False, False, False, False,  True, False],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False, False,\n",
      "         False, False, False, False, False,  True]])\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = feat @ feat.transpose(1, 0)\n",
    "print(similarity_matrix)\n",
    "label_matrix = label.unsqueeze(1) == label.unsqueeze(0)\n",
    "print(label_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "positive_matrix = label_matrix.triu(diagonal=1)\n",
    "print(positive_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True, False, False],\n",
       "        [False, False, False, False,  True, False, False, False,  True,  True,\n",
       "         False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False,  True,  True,  True, False, False,\n",
       "         False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "         False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False,  True,  True, False, False,\n",
       "         False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False,  True, False, False,\n",
       "         False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_matrix = label_matrix.logical_not().triu(diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix = similarity_matrix.view(-1)\n",
    "positive_matrix = positive_matrix.view(-1)\n",
    "negative_matrix = negative_matrix.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming label_array is your 1x4 array\n",
    "label_array = torch.tensor([1, 2, 1, 3])  # example label array\n",
    "\n",
    "# Create a 4x4 matrix where each row is filled with the corresponding label\n",
    "# label_matrix = torch.zeros((4, 4))\n",
    "\n",
    "# Fill the matrix\n",
    "# label_matrix[torch.arange(4), label_array] = 1\n",
    "label_matrix = (torch.arange(4) == label_array[:, None]).float()\n",
    "print(label_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_array.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mim(pred, target, patch_mask):\n",
    "    \"\"\"\n",
    "    pred: [N, L, p*p*3]  L = patches # masks generated by hogloss\n",
    "    mask: [N, (W/block size) * (H / block size)], 0 is keep, 1 is remove, \n",
    "    \"\"\"\n",
    "    #compute loss:\n",
    "    B, N, C = pred.shape\n",
    "    H = W = int(N**0.5)\n",
    "    target = target.permute(0, 2, 3, 1) #BxPxPx(Cxbins)\n",
    "    print(\"\\n\\t pred shape :\", pred.shape)\n",
    "    print(\"\\n\\t target shape :\", target.shape)\n",
    "    print(\"\\n\\t mask shape :\", patch_mask.shape)\n",
    "    \n",
    "    mask_size = patch_mask.shape[-1]\n",
    "    if mask_size > W:\n",
    "\n",
    "        target_size, target_channel = target.shape[2], target.shape[3]\n",
    "        target = target.flatten(1, 2) #Bx(h*w)x(C*bin)\n",
    "        patch_mask = torch.repeat_interleave(patch_mask, target_size//mask_size, dim=2)\n",
    "        patch_mask = torch.repeat_interleave(patch_mask, target_size//mask_size, dim=3)\n",
    "        pred = pred.reshape(B, H, W, -1, target_size//H, target_size//W).permute(0, 1, 4, 2, 5, 3).reshape(B, target_size**2, target_channel)\n",
    "    else:\n",
    "        \n",
    "        unfold_size = target.shape[-1] // W\n",
    "        if unfold_size > 0: \n",
    "            target = (\n",
    "                target.unfold(1, unfold_size, unfold_size)\n",
    "                .unfold(2, unfold_size, unfold_size)\n",
    "                .flatten(1, 2).flatten(2)\n",
    "            )\n",
    "        else: target = target.flatten(1, 2).flatten(2)\n",
    "    pred.\n",
    "    print(\"after change\")\n",
    "    print(\"\\npred shape :\", pred.shape)\n",
    "    print(\"\\ntarget shape: \", target.shape)\n",
    "    print(\"\\nmask shape : \", patch_mask.shape)\n",
    "    mim_loss = (pred[patch_mask] - target[patch_mask]) ** 2\n",
    "    mim_loss = mim_loss.mean()\n",
    "    return mim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand((8, 16*12, 108)) #\n",
    "target = torch.rand(8, 27, 16*2, 12*2)\n",
    "mask = torch.rand(8, 16*12*4)\n",
    "compute_mim(x, target, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 27, 32, 24])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flatten(1).reshape(target.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 768, 27]), torch.Size([8, 768]), torch.Size([8, 27, 32, 24]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "# --------------------------------------------------------\n",
    "# References:\n",
    "# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# MAE: https://github.com/facebookresearch/mae\n",
    "# UM-MAE: https://github.com/implus/UM-MAE\n",
    "# --------------------------------------------------------\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import timm.models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block, DropPath, Mlp\n",
    "\n",
    "from util.pos_embed import get_2d_sincos_pos_embed\n",
    "\n",
    "\n",
    "class MaskedAutoencoderViT(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=1024, depth=24, num_heads=16,\n",
    "                 decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False,\n",
    "                 asymmetric_decoder=False, mask_ratio=0.75, vis_mask_ratio=0.,\n",
    "                 learning_loss=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vis_mask_ratio = vis_mask_ratio\n",
    "        if vis_mask_ratio > 0:\n",
    "            self.vis_mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.learning_loss = learning_loss\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim),\n",
    "                                      requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics (projector, predictor, and loss predictor)\n",
    "        self.decoder_embed_dim = decoder_embed_dim\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim),\n",
    "                                              requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        # reconstructor (e.g., projector at feature-level)\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        # self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size ** 2 * in_chans, bias=True)  # decoder to patch\n",
    "        # self.decoder_pred = nn.Linear(decoder_embed_dim, embed_dim, bias=True)          # dino/ema\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, decoder_embed_dim, bias=True)  # clip\n",
    "\n",
    "        # loss predictor\n",
    "        if self.learning_loss:\n",
    "            self.decoder_blocks_losspred = nn.ModuleList([\n",
    "                Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None,\n",
    "                      norm_layer=norm_layer)\n",
    "                for i in range(decoder_depth)])\n",
    "            self.decoder_norm_losspred = norm_layer(decoder_embed_dim)\n",
    "            self.decoder_pred_losspred = nn.Linear(decoder_embed_dim, patch_size ** 2 * in_chans, bias=True)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches ** .5),\n",
    "                                            cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1],\n",
    "                                                    int(self.patch_embed.num_patches ** .5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        if hasattr(self, 'vis_mask_token'):\n",
    "            torch.nn.init.normal_(self.vis_mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p ** 2 * 3))\n",
    "        # x = rearrange(imgs, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size[0]\n",
    "        h = w = int(x.shape[1] ** .5)\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def forward_encoder(self, x, mask):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        N, _, D = x.shape\n",
    "        x = x[~mask].reshape(N, -1, D)\n",
    "\n",
    "        if self.vis_mask_ratio > 0:\n",
    "            vis_mask_token = self.vis_mask_token + self.pos_embed[:, 1:, :]\n",
    "            vis_mask_token = vis_mask_token.expand(N, -1, -1)\n",
    "            vis_mask_token = vis_mask_token[~mask].reshape(N, -1, D)\n",
    "            L = x.size(1)\n",
    "            noise = torch.rand(N, L, device=x.device)\n",
    "            ids_restore = torch.argsort(noise, dim=1)\n",
    "\n",
    "            len_keep = int(L * (1 - self.vis_mask_ratio))\n",
    "            vis_mask = torch.ones([N, L], device=x.device)\n",
    "            vis_mask[:, :len_keep] = 0\n",
    "            vis_mask = torch.gather(vis_mask, dim=1, index=ids_restore).unsqueeze(-1)\n",
    "\n",
    "            x = x * (1. - vis_mask) + vis_mask_token * vis_mask\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_decoder(self, x, mask):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        x_vis = x[:, 1:, :]\n",
    "        N, _, D = x_vis.shape\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        expand_pos_embed = self.decoder_pos_embed[:, 1:, :].expand(N, -1, -1)\n",
    "        pos_vis = expand_pos_embed[~mask].reshape(N, -1, D)\n",
    "        pos_mask = expand_pos_embed[mask].reshape(N, -1, D)\n",
    "\n",
    "        x_ = torch.cat([x_vis + pos_vis, self.mask_token + pos_mask], dim=1)\n",
    "\n",
    "        # add cls_token + decoder_pos_embed\n",
    "        x = torch.cat([x[:, :1, :] + self.decoder_pos_embed[:, :1, :], x_], dim=1)\n",
    "        loss_pred = x.clone()\n",
    "\n",
    "        # apply reconstructor\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        if self.learning_loss:\n",
    "            # apply loss predictor\n",
    "            for blk in self.decoder_blocks_losspred:\n",
    "                loss_pred = blk(loss_pred)\n",
    "            loss_pred = self.decoder_norm_losspred(loss_pred)\n",
    "            loss_pred = self.decoder_pred_losspred(loss_pred)\n",
    "            loss_pred = loss_pred[:, 1:, :]  # (N, L, 1)\n",
    "\n",
    "            return x, pos_mask.shape[1], loss_pred.mean(dim=-1)\n",
    "\n",
    "        return x, pos_mask.shape[1]\n",
    "\n",
    "    def forward_loss(self, pred, target, mask):\n",
    "        \"\"\"\n",
    "        pred: [N, mask, D]\n",
    "        target: [N, L, D]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        N, _, D = target.shape\n",
    "        target = target[mask].reshape(N, -1, D)\n",
    "\n",
    "        pred = torch.nn.functional.normalize(pred, p=2, dim=-1)\n",
    "        target = torch.nn.functional.normalize(target, p=2, dim=-1)\n",
    "        loss = ((pred - target) ** 2).sum(dim=-1)\n",
    "\n",
    "        return {'mean': loss.mean(), 'matrix': loss}\n",
    "\n",
    "    def forward(self, imgs, mask):\n",
    "        latent = self.forward_encoder(imgs, mask)  # returned mask may change\n",
    "\n",
    "        if self.learning_loss:\n",
    "            pred, mask_num, loss_pred = self.forward_decoder(latent, mask)  # [N, L, p*p*3]\n",
    "        else:\n",
    "            pred, mask_num = self.forward_decoder(latent, mask)\n",
    "        # loss = self.forward_loss(imgs, pred[:, -mask_num:], mask)\n",
    "        # return loss, pred, mask\n",
    "        out = {\n",
    "            'pix_pred': pred,\n",
    "            'mask': mask,\n",
    "            'mask_num': mask_num,\n",
    "            'features': latent,\n",
    "        }\n",
    "\n",
    "        if self.learning_loss:\n",
    "            out['loss_pred'] = loss_pred\n",
    "\n",
    "        return out\n",
    "\n",
    "    def generate_mask(self, loss_pred, mask_ratio=0.75, images=None,  guide=True, epoch=0, total_epoch=200):\n",
    "        loss_pred = loss_pred.squeeze()\n",
    "        N, L = loss_pred.shape\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        ids_shuffle_loss = torch.argsort(loss_pred, dim=1)  # (N, L)\n",
    "\n",
    "        # keep `keep_ratio` loss and `1 - keep_ratio` random\n",
    "        keep_ratio = 0.2\n",
    "        ids_shuffle = torch.zeros_like(ids_shuffle_loss, device=loss_pred.device).int()\n",
    "\n",
    "        if guide:\n",
    "            keep_ratio = float((epoch + 1) / total_epoch) * 0.5\n",
    "\n",
    "        ## top 0 -> 0.5\n",
    "        if int((L - len_keep) * keep_ratio) <= 0:\n",
    "            # random\n",
    "            noise = torch.randn(N, L, device=loss_pred.device)\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                ## mask top `keep_ratio` loss and `1 - keep_ratio` random\n",
    "                len_loss = int((L - len_keep) * keep_ratio)\n",
    "                ids_shuffle[i, -len_loss:] = ids_shuffle_loss[i, -len_loss:]\n",
    "\n",
    "                temp = torch.arange(L, device=loss_pred.device)\n",
    "                deleted = np.delete(temp.cpu().numpy(), ids_shuffle[i, -len_loss:].cpu().numpy())\n",
    "                np.random.shuffle(deleted)\n",
    "                ids_shuffle[i, :(L - len_loss)] = torch.LongTensor(deleted).to(loss_pred.device)\n",
    "\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # generate mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=loss_pred.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get final mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward_learning_loss(self, loss_pred, mask, loss_target, relative=False):\n",
    "        \"\"\"\n",
    "        loss_pred: [N, L, 1]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        loss_target: [N, L]\n",
    "        \"\"\"\n",
    "        # N, L = loss_target.shape\n",
    "        # loss_pred = loss_pred[mask].reshape(N, L)\n",
    "        assert self.learning_loss\n",
    "\n",
    "        if relative:\n",
    "            # binary classification for LxL\n",
    "            labels_positive = loss_target.unsqueeze(1) > loss_target.unsqueeze(2)\n",
    "            labels_negative = loss_target.unsqueeze(1) < loss_target.unsqueeze(2)\n",
    "            labels_valid = labels_positive + labels_negative\n",
    "\n",
    "            loss_matrix = loss_pred.unsqueeze(1) - loss_pred.unsqueeze(2)\n",
    "            loss = - labels_positive.int() * torch.log(torch.sigmoid(loss_matrix) + 1e-6) \\\n",
    "                   - labels_negative.int() * torch.log(1 - torch.sigmoid(loss_matrix) + 1e-6)\n",
    "\n",
    "            return loss.sum() / labels_valid.sum()\n",
    "\n",
    "        else:\n",
    "            # normalize by each image\n",
    "            mean = loss_target.mean(dim=1, keepdim=True)\n",
    "            var = loss_target.var(dim=1, keepdim=True)\n",
    "            loss_target = (loss_target - mean) / (var + 1.e-6) ** .5  # [N, L, 1]\n",
    "\n",
    "            loss = (loss_pred - loss_target) ** 2\n",
    "            loss = loss.mean()\n",
    "            return loss\n",
    "\n",
    "\n",
    "def mae_vit_base_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=768, depth=12, num_heads=12,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_large_patch16_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mae_vit_huge_patch14_dec512d8b(**kwargs):\n",
    "    model = MaskedAutoencoderViT(\n",
    "        patch_size=14, embed_dim=1280, depth=32, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_mask( loss_pred, mask_ratio=0.75, images=None,  guide=True, epoch=0, total_epoch=200):\n",
    "    loss_pred = loss_pred.squeeze()\n",
    "    N, L = loss_pred.shape\n",
    "    len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "    ids_shuffle_loss = torch.argsort(loss_pred, dim=1)  # (N, L)\n",
    "    print(\"ids_shuffle_loss\")\n",
    "    print(ids_shuffle_loss)\n",
    "    # keep `keep_ratio` loss and `1 - keep_ratio` random\n",
    "    keep_ratio = 0.2\n",
    "    ids_shuffle = torch.zeros_like(ids_shuffle_loss, device=loss_pred.device).int()\n",
    "    print(\"ids_shuffle\")\n",
    "    print(ids_shuffle)\n",
    "\n",
    "    if guide:\n",
    "        keep_ratio = float((epoch + 1) / total_epoch) * 0.5\n",
    "\n",
    "    ## top 0 -> 0.5\n",
    "    if False and int((L - len_keep) * keep_ratio) <= 0:\n",
    "        print(\"random\")\n",
    "        # random\n",
    "        noise = torch.randn(N, L, device=loss_pred.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "    else:\n",
    "        for i in range(N):\n",
    "            ## mask top `keep_ratio` loss and `1 - keep_ratio` random\n",
    "            len_loss = int((L - len_keep) * keep_ratio)\n",
    "            print(\"len losss ==\", len_loss, \"keep ratio ==\", keep_ratio )\n",
    "            ids_shuffle[i, -len_loss:] = ids_shuffle_loss[i, -len_loss:]\n",
    "\n",
    "            temp = torch.arange(L, device=loss_pred.device)\n",
    "            deleted = np.delete(temp.cpu().numpy(), ids_shuffle[i, -len_loss:].cpu().numpy())\n",
    "            print(\"delete Tensor\")\n",
    "            print(deleted)\n",
    "            np.random.shuffle(deleted)\n",
    "            ids_shuffle[i, :(L - len_loss)] = torch.LongTensor(deleted).to(loss_pred.device)\n",
    "\n",
    "    ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "    # generate mask: 0 is keep, 1 is remove\n",
    "    mask = torch.ones([N, L], device=loss_pred.device)\n",
    "    mask[:, :len_keep] = 0\n",
    "    # unshuffle to get final mask\n",
    "    mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "    return mask\n",
    "\n",
    "pred = torch.rand((1, 4 , 8))\n",
    "print(pred.shape, pred)\n",
    "generate_mask(pred, epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9162, 0.4749, 0.3362, 0.6069, 0.3912, 0.7985, 0.7731, 0.3706, 0.4915,\n",
      "         0.9046, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4569, 0.0728, 0.8442, 0.7757, 0.5863, 0.6298, 0.5279, 0.8872, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "loss = torch.rand(2, 16)\n",
    "loss[0, 10:]=0\n",
    "loss[1, 8:]=0\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = torch.sum(loss > 0, axis=1)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6, 5]), tensor([4, 3]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = L\n",
    "mlm_prob=0.6\n",
    "mmm_mask=0.5\n",
    "num_masked_paths = torch.round(P * mlm_prob + 0.5).clamp_max(P).long()\n",
    "num_hard_masked_paths = torch.round(num_masked_paths * mmm_mask +0.5).clamp_max(num_masked_paths).long()\n",
    "num_masked_paths, num_hard_masked_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  9,  5,  6,  3,  8,  1,  4,  7,  2, 10, 11, 12, 13, 14, 15],\n",
       "        [ 7,  2,  3,  5,  4,  6,  0,  1,  8,  9, 10, 11, 12, 13, 14, 15]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_tensor = torch.argsort(loss, dim=1, descending=True)\n",
    "idx_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss[\u001b[43midx_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hard_masked_paths\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(loss[idx_tensor[:, num_hard_masked_paths:]]\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "loss[idx_tensor[:, num_hard_masked_paths:]] * torch.rand(loss[idx_tensor[:, num_hard_masked_paths:]].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 3, 4, 0, 1, 2, 5, 7],\n",
       "        [0, 3, 4, 1, 7, 6, 2, 5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_tensor[:, num_hard_masked_paths:] = idx_tensor[:, num_hard_masked_paths:][:, torch.randperm(P-num_hard_masked_paths)]\n",
    "idx_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False, False],\n",
      "        [ True,  True,  True, False]])\n",
      "tensor([[ 1,  2, -1, -1],\n",
      "        [ 3,  0,  2, -1]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index -1 is out of bounds for dimension 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(value_tensor)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Use scatter to place the values from value_tensor into result at the positions specified by masked_index\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index -1 is out of bounds for dimension 1 with size 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your tensors\n",
    "index_tensor = torch.tensor([[1,2,3,0],[3,0,2,1]])\n",
    "value_tensor = torch.tensor([[0.1, 0.2, 0.3, 0.4],[0.1, 0.2, 0.3, 0.4]])\n",
    "P = torch.tensor([2, 3])\n",
    "\n",
    "# Create a mask tensor\n",
    "mask = torch.arange(index_tensor.shape[1]).expand(*index_tensor.shape) < P.unsqueeze(1)\n",
    "print(mask)\n",
    "# Apply the mask to the index tensor\n",
    "masked_index = mask * index_tensor + (mask * 1 - 1)\n",
    "print(masked_index)\n",
    "# Create a tensor of zeros with the same shape as value_tensor\n",
    "result = torch.zeros_like(value_tensor)\n",
    "\n",
    "# Use scatter to place the values from value_tensor into result at the positions specified by masked_index\n",
    "result.scatter_(1, masked_index, value_tensor)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 0, 0],\n",
      "        [3, 0, 2, 0]])\n",
      "tensor([[ 0.4000,  0.1000,  0.2000, -1.0000],\n",
      "        [ 0.4000, -1.0000,  0.3000,  0.1000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Your tensors\n",
    "index_tensor = torch.tensor([[1,2,3,0],[3,0,2,1]])\n",
    "value_tensor = torch.tensor([[0.1, 0.2, 0.3, 0.4],[0.1, 0.2, 0.3, 0.4]])\n",
    "P = torch.tensor([2, 3])\n",
    "\n",
    "# Create a mask tensor\n",
    "mask = torch.arange(index_tensor.shape[1]).expand(*index_tensor.shape) < P.unsqueeze(1)\n",
    "\n",
    "# Apply the mask to the index tensor\n",
    "masked_index = mask * index_tensor\n",
    "print(masked_index)\n",
    "# Create a tensor of zeros with the same shape as value_tensor\n",
    "result = torch.zeros_like(value_tensor)\n",
    "\n",
    "# Use scatter to place the values from value_tensor into result at the positions specified by masked_index\n",
    "result.scatter_(1, masked_index, value_tensor)\n",
    "\n",
    "# Replace 0 with -1 in the result tensor\n",
    "result[result == 0] = -1\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
